1. - 1 min
* Hello, I'm Stefan Wiedemann, I'm an archtiect at the fiware foundation

* In the last months, I was tasked with building scalable, production grade deployment scripts for running various fiware components, espacially the context broker. 

* Beside those deployment scripts, we wanted to provide a prove for that abiltiy to scale and publish reliable numbers for it. In result, everyone should be able to choose a configuration, that allows to run the context broker not only in a scalable manner, but also in a cost efficent way. 

* We solved that with a combination of Helm-Charts, leveraging on kubernetes' ability to scale microservices and improvements on the context broker to get it more cloud-ready. 

* To prove the numbers, we used multiple open-source tools for testing and publishing results. 

* In the next 15 minutes, I want to take a closer look on how to get started when building a scalable context broker deployment and an overview on our concrete solution. 

* I will show some numbers that you can expect and start to build on. 

2. - 4-5 min

* The first part when starting to build a scalable deployment is, that you should know about you use-case.

* But what does that mean? 

* Most of the time in software-development, we have to handle trade-offs. 

* If you optimize one thing, it might degrade another functionality or it does at least not scale in the same way. 

* But in order to have every functionality at the same high level of performance, you might need to oversize the installation in a way, that makes it expensive and inefficent to run. 

* To prevent oversizing I would recommend to start with thinking about the following points. 

* Depending on your concrete use-case, there might be more, but those 3 are more or less common in every use-case. 

* As you probably now, the NGSI-LD Api, and therefore also the broker, allow a wide varitiy of retrieval and querying operations and also different methods to write data to it. 

* Because of that, you should for example check if you focus is on multiple parallel high frequent updates or if a lot parallel retrieval requests are more common. 

* The type of request is also important.

* If the typical request is a complex query, you might put more pressure on the database, while for a lot of parallel entity-retrivals the brokers connection handling might be the more limiting factor.

* Its clear that most of the time you will have mixture of everything, but finding the focus and probably first functionality to reach the limits helps to get the right focus for the deployment.

* Next important point would be to look at the kind of updates. 

* The api allows different ways to update an entity, like full updates of single entities, updates of single attributes or even updating multiple of them in a batch.

* This differentiation again is important when searching for the right optimization points.

* If the use-case requires a lot of parallel updates of single entities or attributes, the configuration of connections pools for the broker and the connected database are very important, while fewer but bigger batch updates put more pressure on the brokers memory.

* A common use-case for multiple parallel updates is a scenario where a lot of IoT-devices are directly connected to the broker. 

* Batch updates might come from gateway-devices that are connected to multiple different devices or if another context-source provides information about multiple entities.

* Another important feature to keep in mind when optimizing, is the usage of subscriptions and notifications.

* As you might know, they are a quite important and powerful feature of the ngis-ld-api, but they are also one that can easily harm the performance of the broker if not correctly used and configured. 

* So first of all, you should check if you need them at all. If not, you can skip that section. 

* If they are used, its important to know how the typical subscription will look like. 

* If there are a lot of subscribers, only receiving some data each, the connection pool configuration should be different than if you only have one or few subscribers, like for example in deployments where additionl components like quantum leap or cosmos are connected. 

* Orion provides very fine grained options for configuring the connection pool, that can make a huge difference for a given scenario. 
* I will not concentrate much on those options, but I recommend to take a close look on the documentation for them when optimzing the brokers deployment.

* The last important point for the subscriptions is, that you should be aware of impact that the subscribers performance has on the broker. 

* If the subscribers are slow or unstable, it can also slow down the broker itself. 

* In such cases it might make sense to use some kind of circuit breaker on the cluster, to allow fail-fast for the notifications or put a more stable and performant intermediate component like an mqtt broker in front of the subscribers and use mqtt subscriptions.   

3. - 2min
* Now that we know how our use-case looks like, we can start with optimizing. 

* In order to do that, we not only need to care about the broker, but also about the underlying infrastructre. 

* The infrastructure should be able to scale in the required way and also support managing a highly scaled cluster, espacially in peak situations. 

* Fullfilling such requirements needs a high degree of automation. 

* Therefore we choose kuberentes as the recommended orchestration tool for installations of fiware components. 

* Kubernetes was orginially designed by google and is now an  opensource project, maintained by the cloud native computing foundation.

* Therefore kuberenetes can be run on bare-metal in any given datacenter, but is also offered as a managed service by most of the cloud providers. 

* It provides us an abstraction layer to use the same deployment scripts in different environments. Beside that, it has functionality for automatic up and downscaling and therefore can help with building the cluster in a cost-efficent way.

* Since every deployment of the broker needs some degree of individual configuration, we decided to use Helm-Charts for the deployment scripts. Helm is another CNCF project and provides a templating mechanism for the kubernetes recipies. You can find the chart for the orion-broker at this link.

* Beside those deployment scripts, I would recommend putting some focus on the operational tooling. 

* Running and being able to react on a cluster in high-load scenarios requires proper tooling for logging, monitoring and alerting. 

* Our own clusters are built on openshift to provide those functionality, but there are other options like for example, thanos, prometheus or fluentd available. 

* Additionally, if you run your cluster on a certain cloud provider or one of the hyper-scalers, they often already provide some managed tooling for that. You should make yourself familiar with the tools surrounding the deployment, because it get only harder when it grows.

4. - 2min

* after analyzing the use-case and setting up the infrastructure and the broker, all the assumptions and decisions should be tested. 

* In order to do that, we implemented a set of load-tests, that can be run against any implementation of the ngsi-ld or ngsi-v2 api and allows to test if it is able to handle the planned load and find the boundaries of the installation.

* The tests are built based on the gatling-framework. Gatling is an opensource-framework for load-testing, that supports a high degree of parallelism and offers a domain-specific language to implement relatively easy to read test-scenarios in scala. 

* It provides reports that allow close analyses of the test runs and a configuration mechanism to run scenarios in different sizes. The reports are provided in html and look similar to the example on the bottom of the slide. 

* Our test-implementations are fully available on github. They can be executed via maven or inside a kuberentes cluster with a helm-chart.

* For simulating high-load scenarios, running via kuberenetes is recommended, since it can scale horizontally to enable more parallel clients. The gatling scenarios will be scaled out over multiple pods by the helm chart and report to a central instance in the cluster that aggregates the individual reports and provide the aggregated report at a static fileserver.

* Beside that, if the test are run inside the cluster, it takes away the network of the loadgenerator as a bottleneck. 

* Be aware that in a production use-case, your external loadbalancer and the ingress controllers need to be able to handle the load, too. The test will only go through the internal loadbalancing if run inside the same cluster.

* Beside the test-implementations itself, the github repo also offers reports on testruns executed by us. 

* The test where run on different sizes and configurations of clusters and should provide a reference for choosing the right size of cluster. All reports include a detailed description on the configuration of the infrastructure and the charts used for orion and mongodb. We provide the configuration for database because the orion-context-broker is tightly coupled to its database and therefore both need to be scaled to prevent mongo becoming the bottleneck.

5.
* Short look on the repository

* You will find a an overview for the whole framework. 

* As mentionend before, the test can be run via maven or helm. 

* Since they are normal gatling-implementations, all other ways to run a gatling test can be used. I would recommend taking a look on the gatling documentation if the described options don't fit your needs. 

* A detailed description of the implemented tests can be found here. They are focusing on the most common use-cases for the ngsi-api. If you have discovered more specific scenarios in your use-case analysis, it might make sense to implement additional test that fit better. 

* The first set of tests focuses on write operations. 

* We have tests for the update of single entities and batch updates as described before.

* You can configure a number of details about the test. The number of updates should be choosen big enough to even out possible influences by the network and of the test startup. I will come back to that point when we look at a concrete report.

* Another group of tests is focused on read scenarios. 

* Since we need a dataset to query on, a sample scenario is created at the start of every test and cleaned-up afterwards. 

* The used entities can be found in the json-templates at the doc-folder. Since runtime of queries also depends on the size of the database, you can again define the number of entities to be created. 

* Beside that, all tests support a prefill-parameter, where you can define the number of instances to be created before the test. Be aware that a huge number of prefill instances can drastically increase the runtime of your test.

* The third group of tests is kind of an extension to the update tests and focuses on notifications. 

* It will generate the same gatling reports as the other tests, but also provide some measurements on the duration it takes from an update generated at client-side until its received by the subscriber. 

* In order to take out the subscriber as a bottleneck, we use telegraf and influxdb in the kubernetes-test-setup and assign relativly high resources to them. As shown in the diagram, the duration is measured with a creation-timestamp taken at the client and a received-timestamp taken at telegraf. The results can be analyzed using grafana.

-> reports

* As advertised, we also provide results of load-tests that we did run on our own infrastructure. 

* You can find the results and descrptions here. 

* Right now, we tested installations of 3 different sizes, focusing only on the orion-context-broker and the connected mongo-db. 

* We called them tiny, small and mid, with cluster sizes scaling from 9 CPUs/ 22 GB ram up to 68 CPUs/ 176 GB ram. 

* Be aware that the complete production cluster will be bigger, since the resources for loadbalancing, monitoring alerting and logging are not included here. 

* Addtional, the cluster will also require additional resources for the kuberentes main-nodes. We do not include those resources here, since they are highly reliant on the used kubernetes installation. In general, the relation of cluster-management resources to broker resources will get more favorable in bigger clusters. 

* Lets look at a concrete setup, for example tiny. 

* it starts with a summary about the performance we where able to achieve. 

* As you can see, that the orion-ld broker is able to handle around 1300-1500 updates/s and a retrieval of up to 2000 req/s on only one core.

* The v2-endpoint is able to handle even more, since the computation overhead for handling the linked-data context is not required there. 

* The detailed reports are linked on the corresponding github page here.

* You can find information about the average repsonse times at the top of the chart and more detailed time information for every step of the test in the table below.

* Don't get confused by the notation of active users, thats the standard naming from gatling. In our tests, user corresponds to entity.

* Beside different graphs on the timing of requests and responses, the last overview is espacially intersting if we want to find out the boundaries of the system. 

* It shows the number of parallel responses per second. 

* As you can see, we have a plateau in the middle, wich is the time we are running the concrete tests. 

* The start and end part include the requests for setup and cleanup. In this case, we see that something between 800 and 900 responses/s can be expected. 

* Additionally, for the tests with measuring the notification latency, we provide an image of the grafana chart. 

* In this case, we have a mean latency of 1.3s. More detailed analyses can be done when rerunning the tests on your cluster.

* I addtion to the test results, we provide the information on how to reach them.

* As already mentioned, we run our clusters using openshift. 

* The required helm-commands and optimizations to apply are described here and the applied configuration in form of helm-values files is here.

* I hope this will give provide you a good starting point for your broker installation.

* Thank you very much.